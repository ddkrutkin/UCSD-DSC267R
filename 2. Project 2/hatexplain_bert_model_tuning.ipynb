{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency loading, initial processing"
      ],
      "metadata": {
        "id": "idsyd4qMN900"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJzvC8M2K1wk",
        "outputId": "770138c0-2d38-4845-b866-9ecec75c07d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibility seeding\n",
        "def set_seed(seed=100):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(777)"
      ],
      "metadata": {
        "id": "-LpBE0loLC9_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7BlhOwULPd_",
        "outputId": "fc9f03cd-bfca-4cb8-f2a0-c2d6c2e1e80b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load HateXplain dataset from Hugging Face\n",
        "print(\"Loading the HateXplain dataset...\")\n",
        "try:\n",
        "    dataset = load_dataset(\"hatexplain\")\n",
        "    print(\"Dataset loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e-grwxnLggT",
        "outputId": "e96ac8dc-8175-45cf-803c-4ee4a3c19f1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the HateXplain dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic information about the dataset structure\n",
        "print(\"\\nDataset structure:\")\n",
        "print(f\"Train set size: {len(dataset['train'])}\")\n",
        "print(f\"Validation set size: {len(dataset['validation'])}\")\n",
        "print(f\"Test set size: {len(dataset['test'])}\")\n",
        "\n",
        "# Example item (commented-out to avoid senstive example on github):\n",
        "print(\"\\nSample item from training set:\")\n",
        "sample_item = dataset['train'][0]\n",
        "print(f\"Keys: {sample_item.keys()}\")\n",
        "print(f\"Annotators keys: {sample_item['annotators'].keys()}\")\n",
        "print(f\"Label structure: {sample_item['annotators']['label']}\")\n",
        "#print(f\"Sample text: {' '.join(sample_item['post_tokens'])[:100]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNL2z9OZLjkq",
        "outputId": "5d41d981-623e-492a-ad50-ba44b8273cff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset structure:\n",
            "Train set size: 15383\n",
            "Validation set size: 1922\n",
            "Test set size: 1924\n",
            "\n",
            "Sample item from training set:\n",
            "Keys: dict_keys(['id', 'annotators', 'rationales', 'post_tokens'])\n",
            "Annotators keys: dict_keys(['label', 'annotator_id', 'target'])\n",
            "Label structure: [0, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "ABUfDsEfNbKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the preprocessing function\n",
        "def preprocess_data(dataset_split):\n",
        "    \"\"\"\n",
        "    Process the HateXplain dataset split and extract texts and labels.\n",
        "    In HateXplain, labels are already numeric:\n",
        "    hatespeech (0), normal (1) or offensive (2)\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    for item in tqdm(dataset_split, desc=\"Processing data\"):\n",
        "        # Extract text from tokens\n",
        "        post_tokens = item['post_tokens']\n",
        "        text = \" \".join(post_tokens)\n",
        "\n",
        "        # Get the annotations\n",
        "        annotator_labels = item['annotators']['label']\n",
        "\n",
        "        # Compute majority vote for the label\n",
        "        if len(annotator_labels) > 0:\n",
        "            # Count occurrences of each label\n",
        "            label_counts = {}\n",
        "            for label in annotator_labels:\n",
        "                if label in label_counts:\n",
        "                    label_counts[label] += 1\n",
        "                else:\n",
        "                    label_counts[label] = 1\n",
        "\n",
        "            # Find the majority label\n",
        "            majority_label = max(label_counts.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "            # Add to dataset\n",
        "            texts.append(text)\n",
        "            labels.append(majority_label)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "# Create a custom dataset class\n",
        "class HateXplainDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define training function\n",
        "def train():\n",
        "    # Initialize variables to track best model\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_path = 'best_bert_hatexplain_model.pt'\n",
        "\n",
        "    print(f\"\\nStarting training for {epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            model.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_preds = []\n",
        "        val_true = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                true_labels = labels.cpu().numpy()\n",
        "\n",
        "                val_preds.extend(preds)\n",
        "                val_true.extend(true_labels)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_dataloader)\n",
        "        val_accuracy = accuracy_score(val_true, val_preds)\n",
        "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
        "            val_true, val_preds, average='weighted'\n",
        "        )\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
        "        print(f\"Val F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Saved best model to {best_model_path}\")\n",
        "\n",
        "    return best_model_path\n",
        "\n",
        "# Function to evaluate model on test set\n",
        "def evaluate(model_path):\n",
        "    # Load the best model\n",
        "    print(f\"\\nLoading best model from {model_path}...\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    test_preds = []\n",
        "    test_true = []\n",
        "\n",
        "    print(\"Evaluating on test set...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            true_labels = labels.cpu().numpy()\n",
        "\n",
        "            test_preds.extend(preds)\n",
        "            test_true.extend(true_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(test_true, test_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        test_true, test_preds, average='weighted'\n",
        "    )\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(\n",
        "        test_true,\n",
        "        test_preds,\n",
        "        target_names=['hate', 'normal', 'offensive'],\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(pd.DataFrame(report).transpose())\n",
        "\n",
        "    # Error analysis\n",
        "    error_analysis(test_texts, test_true, test_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, report\n",
        "\n",
        "\n",
        "# Function for error analysis\n",
        "def error_analysis(texts, true_labels, pred_labels):\n",
        "    errors = []\n",
        "    for i in range(len(texts)):\n",
        "        if true_labels[i] != pred_labels[i]:\n",
        "            errors.append({\n",
        "                'text': texts[i],\n",
        "                'true_label': ['hate', 'normal', 'offensive'][true_labels[i]],\n",
        "                'pred_label': ['hate', 'normal', 'offensive'][pred_labels[i]]\n",
        "            })\n",
        "\n",
        "    # Save some examples of errors\n",
        "    error_df = pd.DataFrame(errors)\n",
        "    print(f\"\\nFound {len(errors)} misclassifications out of {len(texts)} samples\")\n",
        "\n",
        "    # Code below shows examples of mis-classifications\n",
        "    # Commented-out to avoid senstive information when viewing notebook\n",
        "    # if len(errors) > 0:\n",
        "    #     print(\"\\nSample of misclassifications:\")\n",
        "    #     sample_size = min(5, len(errors))\n",
        "    #     for i in range(sample_size):\n",
        "    #         print(f\"\\nExample {i+1}:\")\n",
        "    #         print(f\"Text: {error_df.iloc[i]['text'][:100]}...\")\n",
        "    #         print(f\"True label: {error_df.iloc[i]['true_label']}\")\n",
        "    #         print(f\"Predicted label: {error_df.iloc[i]['pred_label']}\")\n",
        "\n",
        "    # Save error analysis to CSV\n",
        "    error_df.to_csv('misclassifications.csv', index=False)\n",
        "    print(\"\\nSaved misclassifications to 'misclassifications.csv'\")"
      ],
      "metadata": {
        "id": "zOyAEKU8L4CW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Driver Cell"
      ],
      "metadata": {
        "id": "osjkSjs4NEPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the dataset\n",
        "print(\"\\nProcessing datasets...\")\n",
        "train_texts, train_labels = preprocess_data(dataset['train'])\n",
        "val_texts, val_labels = preprocess_data(dataset['validation'])\n",
        "test_texts, test_labels = preprocess_data(dataset['test'])\n",
        "\n",
        "# Verify dataset sizes\n",
        "print(f\"\\nProcessed dataset sizes:\")\n",
        "print(f\"Train samples: {len(train_texts)}\")\n",
        "print(f\"Validation samples: {len(val_texts)}\")\n",
        "print(f\"Test samples: {len(test_texts)}\")\n",
        "\n",
        "# Check label distribution\n",
        "train_label_dist = pd.Series(train_labels).value_counts(normalize=True)\n",
        "print(\"\\nTraining label distribution:\")\n",
        "print(train_label_dist)\n",
        "print(\"Label meanings: 0 = hatespeech, 1 = normal, 2 = offensive\")\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "print(\"\\nInitializing BERT model and tokenizer...\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=3,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "# Create datasets\n",
        "print(\"Creating PyTorch datasets...\")\n",
        "train_dataset = HateXplainDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = HateXplainDataset(val_texts, val_labels, tokenizer)\n",
        "test_dataset = HateXplainDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "print(f\"Dataset sizes:\")\n",
        "print(f\"Train: {len(train_dataset)}\")\n",
        "print(f\"Validation: {len(val_dataset)}\")\n",
        "print(f\"Test: {len(test_dataset)}\")\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "print(\"\\nCreating dataloaders...\")\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Training hyperparameters\n",
        "print(\"\\nSetting up training parameters...\")\n",
        "epochs = 4\n",
        "learning_rate = 2e-5\n",
        "warmup_steps = 0\n",
        "weight_decay = 0.01\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "try:\n",
        "    print(\"\\nStarting fine-tuning process...\")\n",
        "    best_model_path = train()\n",
        "    print(\"\\nEvaluating model on test set...\")\n",
        "    accuracy, precision, recall, f1, report = evaluate(best_model_path)\n",
        "\n",
        "    print(\"\\nFinished model tuning and evaluation\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF7zB8K4NHDv",
        "outputId": "88fba7b3-1008-46ab-fccd-4536bf0d7f78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing data: 100%|██████████| 15383/15383 [00:04<00:00, 3537.72it/s]\n",
            "Processing data: 100%|██████████| 1922/1922 [00:00<00:00, 3079.52it/s]\n",
            "Processing data: 100%|██████████| 1924/1924 [00:00<00:00, 3428.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed dataset sizes:\n",
            "Train samples: 15383\n",
            "Validation samples: 1922\n",
            "Test samples: 1924\n",
            "\n",
            "Training label distribution:\n",
            "1    0.406358\n",
            "0    0.308652\n",
            "2    0.284990\n",
            "Name: proportion, dtype: float64\n",
            "Label meanings: 0 = hatespeech, 1 = normal, 2 = offensive\n",
            "\n",
            "Initializing BERT model and tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating PyTorch datasets...\n",
            "Dataset sizes:\n",
            "Train: 15383\n",
            "Validation: 1922\n",
            "Test: 1924\n",
            "\n",
            "Creating dataloaders...\n",
            "\n",
            "Setting up training parameters...\n",
            "\n",
            "Starting fine-tuning process...\n",
            "\n",
            "Starting training for 4 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/4: 100%|██████████| 481/481 [11:25<00:00,  1.43s/it, loss=0.813]\n",
            "Validation: 100%|██████████| 61/61 [00:31<00:00,  1.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/4\n",
            "Train Loss: 0.7918\n",
            "Val Loss: 0.7143\n",
            "Val Accuracy: 0.6873\n",
            "Val F1 Score: 0.6791\n",
            "Saved best model to best_bert_hatexplain_model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/4: 100%|██████████| 481/481 [11:24<00:00,  1.42s/it, loss=0.423]\n",
            "Validation: 100%|██████████| 61/61 [00:31<00:00,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/4\n",
            "Train Loss: 0.6283\n",
            "Val Loss: 0.7202\n",
            "Val Accuracy: 0.6733\n",
            "Val F1 Score: 0.6774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/4: 100%|██████████| 481/481 [11:24<00:00,  1.42s/it, loss=0.5]\n",
            "Validation: 100%|██████████| 61/61 [00:31<00:00,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/4\n",
            "Train Loss: 0.5120\n",
            "Val Loss: 0.7610\n",
            "Val Accuracy: 0.6805\n",
            "Val F1 Score: 0.6713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/4: 100%|██████████| 481/481 [11:24<00:00,  1.42s/it, loss=0.424]\n",
            "Validation: 100%|██████████| 61/61 [00:31<00:00,  1.94it/s]\n",
            "<ipython-input-6-eb3caf478545>:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/4\n",
            "Train Loss: 0.4082\n",
            "Val Loss: 0.8235\n",
            "Val Accuracy: 0.6790\n",
            "Val F1 Score: 0.6757\n",
            "\n",
            "Evaluating model on test set...\n",
            "\n",
            "Loading best model from best_bert_hatexplain_model.pt...\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 61/61 [00:31<00:00,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "Accuracy: 0.6944\n",
            "Precision: 0.6839\n",
            "Recall: 0.6944\n",
            "F1 Score: 0.6825\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score      support\n",
            "hate           0.733846  0.803030  0.766881   594.000000\n",
            "normal         0.707263  0.809463  0.754919   782.000000\n",
            "offensive      0.596306  0.412409  0.487594   548.000000\n",
            "accuracy       0.694387  0.694387  0.694387     0.694387\n",
            "macro avg      0.679138  0.674967  0.669798  1924.000000\n",
            "weighted avg   0.683867  0.694387  0.682472  1924.000000\n",
            "\n",
            "Found 588 misclassifications out of 1924 samples\n",
            "\n",
            "Saved misclassifications to 'misclassifications.csv'\n",
            "\n",
            "Finished model tuning and evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}